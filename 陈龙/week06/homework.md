# 作业整理

## 作业1 使用pytorch实现单层transformer
**实现文件：** `./week06/my_transformer.py`

## 作业2 计算bert-base-chinese的参数量
### 配置如：`./week06/config.json`
### 参数量计算

#### 1. 嵌入层参数量
- 词嵌入：21128 × 768
- 位置嵌入：512 × 768  
- 类型嵌入：2 × 768

**总参数量：** 21128 × 768 + 512 × 768 + 2 × 768 = 16,621,056

#### 2. Transformer层参数量（单层）
包含自注意力层、前向传播层和两个LayerNorm：

- **自注意力层：**
  - Q/W/K/V权重：4 × (768 × 768)
  - Q/W/K/V偏置：4 × 768
  - 输出权重：768 × 768
  - 输出偏置：768

- **LayerNorm（2个）：** 2 × (768 + 768)

- **前向传播层：**
  - 第一层：768 × 3072 + 3072
  - 第二层：3072 × 768 + 768

**单层参数量：** (768 × 768 + 768) × 4 + 768 + 768 + 768 × 3072 + 3072 + 3072 × 768 + 768 + 768 + 768 = 7,087,872

**12层总参数量：** 12 × 7,087,872 = 85,054,464

#### 3. 池化层参数量
- 权重：768 × 768
- 偏置：768

**参数量：** 768 × 768 + 768 = 590,592

### 总参数量
**合计：** 16,621,056 + 85,054,464 + 590,592 = 102,266,112




