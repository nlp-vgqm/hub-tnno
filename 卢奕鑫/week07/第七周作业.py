import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

# è¯»å–æ•°æ®
print("=" * 60)
print("åŠ è½½æ•°æ®é›†...")
df = pd.read_csv('æ–‡æœ¬åˆ†ç±»ç»ƒä¹ .csv')
print(f"æ•°æ®é›†å½¢çŠ¶: {df.shape}")

# ==================== æ•°æ®åˆ†æ ====================
print("\n" + "=" * 60)
print("æ•°æ®åˆ†æ")
print("=" * 60)

# 1. æ­£è´Ÿæ ·æœ¬æ•°
positive_count = df['label'].sum()
negative_count = len(df) - positive_count
print(f"æ­£æ ·æœ¬æ•°(å¥½è¯„): {positive_count}")
print(f"è´Ÿæ ·æœ¬æ•°(å·®è¯„): {negative_count}")
print(f"æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: {positive_count / negative_count:.2f}:1")

# 2. æ–‡æœ¬é•¿åº¦åˆ†æ
df['text_length'] = df['review'].apply(len)
print(f"\næ–‡æœ¬å¹³å‡é•¿åº¦: {df['text_length'].mean():.2f} å­—ç¬¦")
print(f"æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ: min={df['text_length'].min()}, "
      f"max={df['text_length'].max()}, median={df['text_length'].median()}")

# ==================== æ•°æ®é¢„å¤„ç† ====================
print("\n" + "=" * 60)
print("æ•°æ®é¢„å¤„ç†")
print("=" * 60)

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
X = df['review'].values
y = df['label'].values
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
print(f"éªŒè¯é›†å¤§å°: {len(X_val)}")

# æ–¹æ³•1ï¼šä½¿ç”¨è¯è¢‹æ¨¡å‹ï¼ˆé€‚åˆMLPï¼‰
print("\næ–¹æ³•1: ä½¿ç”¨TF-IDFç‰¹å¾ï¼ˆé€‚åˆMLPæ¨¡å‹ï¼‰...")
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer_tfidf = TfidfVectorizer(max_features=3000, max_df=0.95, min_df=2)
X_train_tfidf = vectorizer_tfidf.fit_transform(X_train).toarray()
X_val_tfidf = vectorizer_tfidf.transform(X_val).toarray()
print(f"TF-IDFç‰¹å¾ç»´åº¦: {X_train_tfidf.shape[1]}")

# æ–¹æ³•2ï¼šä½¿ç”¨åºåˆ—è¡¨ç¤ºï¼ˆé€‚åˆCNN/LSTM/Transformerï¼‰
print("\næ–¹æ³•2: ä½¿ç”¨åºåˆ—è¡¨ç¤ºï¼ˆé€‚åˆCNN/LSTM/Transformeræ¨¡å‹ï¼‰...")

# åˆ›å»ºè¯æ±‡è¡¨
vocab = {}
word_to_idx = {}
idx_to_word = {}
vocab_size = 5000  # é™åˆ¶è¯æ±‡è¡¨å¤§å°

# æ„å»ºè¯æ±‡è¡¨
print("æ„å»ºè¯æ±‡è¡¨...")
from collections import Counter

all_words = []
for text in X_train:
    all_words.extend(str(text).split())

word_counts = Counter(all_words)
common_words = word_counts.most_common(vocab_size - 2)  # ä¿ç•™ä½ç½®ç»™PADå’ŒUNK

# å»ºç«‹è¯æ±‡æ˜ å°„
word_to_idx['<PAD>'] = 0
word_to_idx['<UNK>'] = 1
idx_to_word[0] = '<PAD>'
idx_to_word[1] = '<UNK>'

for idx, (word, _) in enumerate(common_words, start=2):
    word_to_idx[word] = idx
    idx_to_word[idx] = word


# æ–‡æœ¬è½¬åºåˆ—
def text_to_sequence(text, max_len=100):
    words = str(text).split()
    sequence = []
    for word in words[:max_len]:
        sequence.append(word_to_idx.get(word, word_to_idx['<UNK>']))
    # å¡«å……æˆ–æˆªæ–­
    if len(sequence) < max_len:
        sequence += [word_to_idx['<PAD>']] * (max_len - len(sequence))
    else:
        sequence = sequence[:max_len]
    return sequence


# è½¬æ¢æ‰€æœ‰æ–‡æœ¬
max_sequence_len = 100
X_train_seq = np.array([text_to_sequence(text, max_sequence_len) for text in X_train])
X_val_seq = np.array([text_to_sequence(text, max_sequence_len) for text in X_val])

print(f"åºåˆ—é•¿åº¦: {max_sequence_len}")
print(f"è¯æ±‡è¡¨å¤§å°: {len(word_to_idx)}")
print(f"è®­ç»ƒé›†åºåˆ—å½¢çŠ¶: {X_train_seq.shape}")
print(f"éªŒè¯é›†åºåˆ—å½¢çŠ¶: {X_val_seq.shape}")

# ==================== æ·±åº¦å­¦ä¹ æ¨¡å‹ ====================
print("\n" + "=" * 60)
print("æ„å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹")
print("=" * 60)


# 1. MLPæ¨¡å‹ï¼ˆä½¿ç”¨TF-IDFç‰¹å¾ï¼‰
class MLPModel(nn.Module):
    def __init__(self, input_dim, hidden_dim=256, num_classes=2):
        super(MLPModel, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, x):
        return self.model(x)


# 2. CNNæ¨¡å‹ï¼ˆä½¿ç”¨åºåˆ—ç‰¹å¾ï¼‰
class CNNModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, num_classes=2):
        super(CNNModel, self).__init__()

        # åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)

        # å·ç§¯å±‚ - ä½¿ç”¨ä¸åŒå°ºå¯¸çš„å·ç§¯æ ¸
        self.convs = nn.ModuleList([
            nn.Conv2d(1, 128, kernel_size=(k, embed_dim))
            for k in [3, 4, 5]
        ])

        # Dropout
        self.dropout = nn.Dropout(0.5)

        # å…¨è¿æ¥å±‚
        self.fc = nn.Linear(128 * 3, num_classes)

        # æ¿€æ´»å‡½æ•°
        self.relu = nn.ReLU()

    def forward(self, x):
        # x shape: [batch_size, seq_len]
        embedded = self.embedding(x)  # [batch_size, seq_len, embed_dim]
        embedded = embedded.unsqueeze(1)  # [batch_size, 1, seq_len, embed_dim]

        # ä¸åŒå°ºå¯¸çš„å·ç§¯
        conv_outputs = []
        for conv in self.convs:
            conv_out = self.relu(conv(embedded)).squeeze(3)  # [batch_size, 128, seq_len-k+1]
            pool_out = torch.max(conv_out, dim=2)[0]  # [batch_size, 128]
            conv_outputs.append(pool_out)

        # æ‹¼æ¥ç‰¹å¾
        cat = torch.cat(conv_outputs, dim=1)  # [batch_size, 128*3]

        # Dropoutå’Œå…¨è¿æ¥
        cat = self.dropout(cat)
        output = self.fc(cat)

        return output


# 3. LSTMæ¨¡å‹ï¼ˆä½¿ç”¨åºåˆ—ç‰¹å¾ï¼‰
class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, num_classes=2):
        super(LSTMModel, self).__init__()

        # åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)

        # LSTMå±‚
        self.lstm = nn.LSTM(embed_dim, hidden_dim,
                            batch_first=True,
                            bidirectional=True,
                            num_layers=2,
                            dropout=0.3)

        # å…¨è¿æ¥å±‚
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        # x shape: [batch_size, seq_len]
        embedded = self.embedding(x)  # [batch_size, seq_len, embed_dim]

        # LSTM
        lstm_out, (hidden, cell) = self.lstm(embedded)

        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]  # [batch_size, hidden_dim*2]

        # å…¨è¿æ¥
        output = self.fc(last_output)

        return output


# 4. ç®€å•çš„Transformeræ¨¡å‹
class TransformerModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_classes=2):
        super(TransformerModel, self).__init__()

        # åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)

        # ä½ç½®ç¼–ç 
        self.pos_encoder = nn.Parameter(torch.randn(1, max_sequence_len, embed_dim) * 0.01)

        # Transformerç¼–ç å±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=256,
            dropout=0.3,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)

        # å…¨è¿æ¥å±‚
        self.fc = nn.Sequential(
            nn.Linear(embed_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        # åµŒå…¥å’Œä½ç½®ç¼–ç 
        embedded = self.embedding(x) + self.pos_encoder

        # åˆ›å»ºæ³¨æ„åŠ›æ©ç ï¼ˆå¿½ç•¥PADæ ‡è®°ï¼‰
        mask = (x == 0)

        # Transformerç¼–ç 
        encoded = self.transformer_encoder(embedded, src_key_padding_mask=mask)

        # å–ç¬¬ä¸€ä¸ªä½ç½®çš„è¾“å‡ºï¼ˆç±»ä¼¼BERTçš„[CLS]ï¼‰
        first_token = encoded[:, 0, :]

        # åˆ†ç±»
        output = self.fc(first_token)

        return output


# è®­ç»ƒå‡½æ•°
def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=10, model_name='Model'):
    print(f"\nè®­ç»ƒ{model_name}...")
    model = model.to(device)

    # è®°å½•è®­ç»ƒè¿‡ç¨‹
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': []
    }

    start_time = time.time()

    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = output.max(1)
            train_total += target.size(0)
            train_correct += predicted.eq(target).sum().item()

        train_loss = train_loss / len(train_loader)
        train_acc = 100. * train_correct / train_total
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)

                val_loss += loss.item()
                _, predicted = output.max(1)
                val_total += target.size(0)
                val_correct += predicted.eq(target).sum().item()

        val_loss = val_loss / len(val_loader)
        val_acc = 100. * val_correct / val_total
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        if (epoch + 1) % 5 == 0 or epoch == 0 or epoch == epochs - 1:
            print(f'Epoch {epoch + 1}/{epochs}: '
                  f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '
                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')

    training_time = time.time() - start_time

    return model, history, training_time


# è¯„ä¼°å‡½æ•° - ä¸ºä¸åŒæ¨¡å‹ç±»å‹åˆ†åˆ«å¤„ç†
def evaluate_model(model, X_val, y_val, device, model_type='mlp'):
    model.eval()

    if model_type == 'mlp':
        # MLPä½¿ç”¨FloatTensor
        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))
    else:
        # å…¶ä»–æ¨¡å‹ä½¿ç”¨LongTensorï¼ˆåºåˆ—æ•°æ®ï¼‰
        val_dataset = TensorDataset(torch.LongTensor(X_val), torch.LongTensor(y_val))

    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            _, predictions = outputs.max(1)

            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(target.cpu().numpy())

    accuracy = accuracy_score(all_targets, all_predictions)
    f1 = f1_score(all_targets, all_predictions)

    return accuracy, f1, np.array(all_predictions)


# æ¨¡å‹é¢„æµ‹é€Ÿåº¦æµ‹è¯•
def test_prediction_speed(model, test_data, device, model_name, model_type='mlp', num_tests=50):
    print(f"æµ‹è¯•{model_name}é¢„æµ‹é€Ÿåº¦...")
    model.eval()

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    if model_type == 'mlp':
        test_tensor = torch.FloatTensor(test_data).to(device)
    else:
        test_tensor = torch.LongTensor(test_data).to(device)

    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(5):
            _ = model(test_tensor[:10])

    # å®é™…æµ‹è¯•
    start_time = time.time()
    with torch.no_grad():
        for _ in range(num_tests):
            outputs = model(test_tensor)

    total_time = time.time() - start_time
    avg_time_per_sample = (total_time / num_tests) / len(test_tensor) * 1000  # æ¯«ç§’

    print(f"å¹³å‡é¢„æµ‹æ—¶é—´: {avg_time_per_sample:.2f} æ¯«ç§’/æ ·æœ¬")
    return avg_time_per_sample


# ==================== ä¸»è®­ç»ƒæµç¨‹ ====================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

results = []

# 1. è®­ç»ƒMLPæ¨¡å‹ï¼ˆä½¿ç”¨TF-IDFç‰¹å¾ï¼‰
print("\n" + "=" * 60)
print("1. è®­ç»ƒMLPæ¨¡å‹")
print("=" * 60)

# å‡†å¤‡æ•°æ®
X_train_mlp = torch.FloatTensor(X_train_tfidf)
X_val_mlp = torch.FloatTensor(X_val_tfidf)
y_train_tensor = torch.LongTensor(y_train)
y_val_tensor = torch.LongTensor(y_val)

train_dataset_mlp = TensorDataset(X_train_mlp, y_train_tensor)
val_dataset_mlp = TensorDataset(X_val_mlp, y_val_tensor)
train_loader_mlp = DataLoader(train_dataset_mlp, batch_size=32, shuffle=True)
val_loader_mlp = DataLoader(val_dataset_mlp, batch_size=32, shuffle=False)

# åˆ›å»ºå’Œè®­ç»ƒMLPæ¨¡å‹
mlp_model = MLPModel(input_dim=X_train_tfidf.shape[1])
optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

trained_mlp, mlp_history, mlp_train_time = train_model(
    mlp_model, train_loader_mlp, val_loader_mlp, optimizer, criterion,
    device, epochs=10, model_name='MLP'
)

# è¯„ä¼°MLP
mlp_accuracy, mlp_f1, mlp_predictions = evaluate_model(
    trained_mlp, X_val_tfidf, y_val, device, model_type='mlp'
)
mlp_speed = test_prediction_speed(trained_mlp, X_val_tfidf, device, 'MLP', model_type='mlp')

# ä¿å­˜MLPç»“æœ
results.append({
    'æ¨¡å‹': 'MLP',
    'è®­ç»ƒæ—¶é—´(ç§’)': round(mlp_train_time, 2),
    'éªŒè¯å‡†ç¡®ç‡(%)': round(mlp_accuracy * 100, 2),
    'F1åˆ†æ•°': round(mlp_f1, 4),
    'é¢„æµ‹é€Ÿåº¦(ms/æ ·æœ¬)': round(mlp_speed, 2),
    'å‚æ•°é‡(M)': round(sum(p.numel() for p in mlp_model.parameters()) / 1e6, 3)
})

# 2. è®­ç»ƒCNNæ¨¡å‹ï¼ˆä½¿ç”¨åºåˆ—ç‰¹å¾ï¼‰
print("\n" + "=" * 60)
print("2. è®­ç»ƒCNNæ¨¡å‹")
print("=" * 60)

# å‡†å¤‡æ•°æ®
train_dataset_cnn = TensorDataset(torch.LongTensor(X_train_seq), torch.LongTensor(y_train))
val_dataset_cnn = TensorDataset(torch.LongTensor(X_val_seq), torch.LongTensor(y_val))
train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=32, shuffle=True)
val_loader_cnn = DataLoader(val_dataset_cnn, batch_size=32, shuffle=False)

# åˆ›å»ºå’Œè®­ç»ƒCNNæ¨¡å‹
cnn_model = CNNModel(vocab_size=len(word_to_idx))
optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

trained_cnn, cnn_history, cnn_train_time = train_model(
    cnn_model, train_loader_cnn, val_loader_cnn, optimizer, criterion,
    device, epochs=10, model_name='CNN'
)

# è¯„ä¼°CNN
cnn_accuracy, cnn_f1, cnn_predictions = evaluate_model(
    trained_cnn, X_val_seq, y_val, device, model_type='seq'
)
cnn_speed = test_prediction_speed(trained_cnn, X_val_seq, device, 'CNN', model_type='seq')

# ä¿å­˜CNNç»“æœ
results.append({
    'æ¨¡å‹': 'CNN',
    'è®­ç»ƒæ—¶é—´(ç§’)': round(cnn_train_time, 2),
    'éªŒè¯å‡†ç¡®ç‡(%)': round(cnn_accuracy * 100, 2),
    'F1åˆ†æ•°': round(cnn_f1, 4),
    'é¢„æµ‹é€Ÿåº¦(ms/æ ·æœ¬)': round(cnn_speed, 2),
    'å‚æ•°é‡(M)': round(sum(p.numel() for p in cnn_model.parameters()) / 1e6, 3)
})

# 3. è®­ç»ƒLSTMæ¨¡å‹ï¼ˆä½¿ç”¨åºåˆ—ç‰¹å¾ï¼‰
print("\n" + "=" * 60)
print("3. è®­ç»ƒLSTMæ¨¡å‹")
print("=" * 60)

# åˆ›å»ºå’Œè®­ç»ƒLSTMæ¨¡å‹
lstm_model = LSTMModel(vocab_size=len(word_to_idx))
optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

trained_lstm, lstm_history, lstm_train_time = train_model(
    lstm_model, train_loader_cnn, val_loader_cnn, optimizer, criterion,
    device, epochs=10, model_name='LSTM'
)

# è¯„ä¼°LSTM
lstm_accuracy, lstm_f1, lstm_predictions = evaluate_model(
    trained_lstm, X_val_seq, y_val, device, model_type='seq'
)
lstm_speed = test_prediction_speed(trained_lstm, X_val_seq, device, 'LSTM', model_type='seq')

# ä¿å­˜LSTMç»“æœ
results.append({
    'æ¨¡å‹': 'LSTM',
    'è®­ç»ƒæ—¶é—´(ç§’)': round(lstm_train_time, 2),
    'éªŒè¯å‡†ç¡®ç‡(%)': round(lstm_accuracy * 100, 2),
    'F1åˆ†æ•°': round(lstm_f1, 4),
    'é¢„æµ‹é€Ÿåº¦(ms/æ ·æœ¬)': round(lstm_speed, 2),
    'å‚æ•°é‡(M)': round(sum(p.numel() for p in lstm_model.parameters()) / 1e6, 3)
})

# 4. è®­ç»ƒTransformeræ¨¡å‹
print("\n" + "=" * 60)
print("4. è®­ç»ƒTransformeræ¨¡å‹")
print("=" * 60)

# åˆ›å»ºå’Œè®­ç»ƒTransformeræ¨¡å‹
transformer_model = TransformerModel(vocab_size=len(word_to_idx))
optimizer = optim.Adam(transformer_model.parameters(), lr=0.0005)
criterion = nn.CrossEntropyLoss()

trained_transformer, transformer_history, transformer_train_time = train_model(
    transformer_model, train_loader_cnn, val_loader_cnn, optimizer, criterion,
    device, epochs=10, model_name='Transformer'
)

# è¯„ä¼°Transformer
transformer_accuracy, transformer_f1, transformer_predictions = evaluate_model(
    trained_transformer, X_val_seq, y_val, device, model_type='seq'
)
transformer_speed = test_prediction_speed(trained_transformer, X_val_seq, device, 'Transformer', model_type='seq')

# ä¿å­˜Transformerç»“æœ
results.append({
    'æ¨¡å‹': 'Transformer',
    'è®­ç»ƒæ—¶é—´(ç§’)': round(transformer_train_time, 2),
    'éªŒè¯å‡†ç¡®ç‡(%)': round(transformer_accuracy * 100, 2),
    'F1åˆ†æ•°': round(transformer_f1, 4),
    'é¢„æµ‹é€Ÿåº¦(ms/æ ·æœ¬)': round(transformer_speed, 2),
    'å‚æ•°é‡(M)': round(sum(p.numel() for p in transformer_model.parameters()) / 1e6, 3)
})

# ==================== ç»“æœå±•ç¤º ====================
print("\n" + "=" * 60)
print("æ¨¡å‹æ€§èƒ½å¯¹æ¯”æ€»ç»“")
print("=" * 60)

# åˆ›å»ºç»“æœDataFrame
results_df = pd.DataFrame(results)

# æ ¼å¼åŒ–è¾“å‡º
print("\n" + "=" * 80)
print("æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨")
print("=" * 80)
print(f"{'æ¨¡å‹':<15} {'å‡†ç¡®ç‡(%)':<12} {'F1åˆ†æ•°':<12} {'è®­ç»ƒæ—¶é—´(s)':<12} {'é¢„æµ‹é€Ÿåº¦(ms)':<12} {'å‚æ•°é‡(M)':<12}")
print("-" * 80)

for _, row in results_df.iterrows():
    print(f"{row['æ¨¡å‹']:<15} {row['éªŒè¯å‡†ç¡®ç‡(%)']:<12.2f} {row['F1åˆ†æ•°']:<12.4f} "
          f"{row['è®­ç»ƒæ—¶é—´(ç§’)']:<12.2f} {row['é¢„æµ‹é€Ÿåº¦(ms/æ ·æœ¬)']:<12.2f} {row['å‚æ•°é‡(M)']:<12.3f}")

print("=" * 80)

# æ‰¾åˆ°æœ€ä½³æ¨¡å‹
best_acc_idx = results_df['éªŒè¯å‡†ç¡®ç‡(%)'].idxmax()
best_f1_idx = results_df['F1åˆ†æ•°'].idxmax()
fastest_idx = results_df['é¢„æµ‹é€Ÿåº¦(ms/æ ·æœ¬)'].idxmin()

best_acc_model = results_df.loc[best_acc_idx]
best_f1_model = results_df.loc[best_f1_idx]
fastest_model = results_df.loc[fastest_idx]

print("\n" + "=" * 60)
print("æœ€ä½³æ¨¡å‹åˆ†æ")
print("=" * 60)
print(f"ğŸ† å‡†ç¡®ç‡æœ€é«˜: {best_acc_model['æ¨¡å‹']} - {best_acc_model['éªŒè¯å‡†ç¡®ç‡(%)']}%")
print(f"ğŸ¯ F1åˆ†æ•°æœ€é«˜: {best_f1_model['æ¨¡å‹']} - {best_f1_model['F1åˆ†æ•°']}")
print(f"âš¡ é¢„æµ‹é€Ÿåº¦æœ€å¿«: {fastest_model['æ¨¡å‹']} - {fastest_model['é¢„æµ‹é€Ÿåº¦(ms/æ ·æœ¬)']}ms/æ ·æœ¬")

# æ‰“å°åˆ†ç±»æŠ¥å‘Šï¼ˆé€‰æ‹©æœ€ä½³æ¨¡å‹ï¼‰
print(f"\nğŸ“Š {best_acc_model['æ¨¡å‹']} è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
if best_acc_model['æ¨¡å‹'] == 'MLP':
    predictions = mlp_predictions
elif best_acc_model['æ¨¡å‹'] == 'CNN':
    predictions = cnn_predictions
elif best_acc_model['æ¨¡å‹'] == 'LSTM':
    predictions = lstm_predictions
else:
    predictions = transformer_predictions

print(classification_report(y_val, predictions, target_names=['å·®è¯„', 'å¥½è¯„']))

# æ‰“å°æ··æ·†çŸ©é˜µ
print("æ··æ·†çŸ©é˜µ:")
cm = confusion_matrix(y_val, predictions)
print(f"TP(çœŸæ­£ä¾‹): {cm[1, 1]}  FP(å‡æ­£ä¾‹): {cm[0, 1]}")
print(f"FN(å‡è´Ÿä¾‹): {cm[1, 0]}  TN(çœŸè´Ÿä¾‹): {cm[0, 0]}")

# ==================== æ€»ç»“åˆ†æ ====================
print("\n" + "=" * 60)
print("æ¨¡å‹ç‰¹ç‚¹æ€»ç»“ä¸å»ºè®®")
print("=" * 60)
print("""
ğŸ“ˆ æ¨¡å‹ç‰¹ç‚¹åˆ†æï¼š

1. MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰:
   - âœ… ä¼˜ç‚¹: è®­ç»ƒå’Œé¢„æµ‹é€Ÿåº¦æœ€å¿«ï¼Œå®ç°ç®€å•
   - âŒ ç¼ºç‚¹: æ— æ³•æ•æ‰åºåˆ—ä¿¡æ¯å’Œä¸Šä¸‹æ–‡å…³ç³»
   - ğŸ“Š é€‚ç”¨: å¯¹é€Ÿåº¦è¦æ±‚é«˜ï¼Œæ–‡æœ¬ç‰¹å¾æ˜ç¡®çš„ä»»åŠ¡

2. CNNï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰:
   - âœ… ä¼˜ç‚¹: èƒ½æœ‰æ•ˆæ•æ‰å±€éƒ¨ç‰¹å¾å’ŒçŸ­è¯­æ¨¡å¼
   - âŒ ç¼ºç‚¹: å¯¹é•¿è·ç¦»ä¾èµ–å¤„ç†æœ‰é™
   - ğŸ“Š é€‚ç”¨: çŸ­æ–‡æœ¬æƒ…æ„Ÿåˆ†æï¼Œå…³é”®è¯è¯†åˆ«

3. LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼‰:
   - âœ… ä¼˜ç‚¹: æ“…é•¿å¤„ç†åºåˆ—ä¾èµ–ï¼Œè®°å¿†é•¿æœŸä¿¡æ¯
   - âŒ ç¼ºç‚¹: è®­ç»ƒè¾ƒæ…¢ï¼Œå‚æ•°é‡å¤§
   - ğŸ“Š é€‚ç”¨: é•¿æ–‡æœ¬åˆ†æï¼Œéœ€è¦è€ƒè™‘ä¸Šä¸‹æ–‡çš„åœºæ™¯

4. Transformer:
   - âœ… ä¼˜ç‚¹: å¹¶è¡Œè®¡ç®—ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¼ºå¤§
   - âŒ ç¼ºç‚¹: éœ€è¦å¤§é‡æ•°æ®ï¼Œè®­ç»ƒæ—¶é—´é•¿
   - ğŸ“Š é€‚ç”¨: å¤æ‚è¯­ä¹‰ç†è§£ï¼Œå¤§è§„æ¨¡æ–‡æœ¬åˆ†ç±»

ğŸ’¡ æ¨èå»ºè®®:
- å¦‚æœè¿½æ±‚é€Ÿåº¦: é€‰æ‹© MLP
- å¦‚æœè¿½æ±‚å¹³è¡¡: é€‰æ‹© CNN æˆ– LSTM
- å¦‚æœæ•°æ®é‡å¤§: é€‰æ‹© Transformer
- ç”µå•†è¯„è®ºåˆ†ç±»: æ¨è CNN æˆ– LSTM
""")

# ä¿å­˜ç»“æœ
print("\n" + "=" * 60)
print("ä¿å­˜å®éªŒç»“æœ")
print("=" * 60)

# ä¿å­˜ç»“æœåˆ°CSV
results_df.to_csv('ç”µå•†è¯„è®ºåˆ†ç±»_æ¨¡å‹å¯¹æ¯”ç»“æœ.csv', index=False, encoding='utf-8-sig')
print("âœ… ç»“æœå·²ä¿å­˜åˆ° 'ç”µå•†è¯„è®ºåˆ†ç±»_æ¨¡å‹å¯¹æ¯”ç»“æœ.csv'")

# ä¿å­˜è¯¦ç»†æŠ¥å‘Š
with open('ç”µå•†è¯„è®ºåˆ†ç±»_å®éªŒæŠ¥å‘Š.txt', 'w', encoding='utf-8') as f:
    f.write("ç”µå•†è¯„è®ºåˆ†ç±»å®éªŒæŠ¥å‘Š\n")
    f.write("=" * 50 + "\n\n")

    f.write("ä¸€ã€å®éªŒåŸºæœ¬ä¿¡æ¯\n")
    f.write(f"- æ•°æ®é›†: {len(df)} æ¡è¯„è®º\n")
    f.write(f"- è®­ç»ƒé›†: {len(X_train)} æ¡\n")
    f.write(f"- éªŒè¯é›†: {len(X_val)} æ¡\n")
    f.write(f"- æ­£æ ·æœ¬: {positive_count} æ¡\n")
    f.write(f"- è´Ÿæ ·æœ¬: {negative_count} æ¡\n")
    f.write(f"- è¯æ±‡è¡¨å¤§å°: {len(word_to_idx)}\n")
    f.write(f"- åºåˆ—é•¿åº¦: {max_sequence_len}\n")
    f.write(f"- TF-IDFç‰¹å¾ç»´åº¦: {X_train_tfidf.shape[1]}\n\n")

    f.write("äºŒã€æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n")
    f.write(results_df.to_string(index=False) + "\n\n")

    f.write("ä¸‰ã€æœ€ä½³æ¨¡å‹\n")
    f.write(f"1. å‡†ç¡®ç‡æœ€é«˜: {best_acc_model['æ¨¡å‹']} ({best_acc_model['éªŒè¯å‡†ç¡®ç‡(%)']}%)\n")
    f.write(f"2. F1åˆ†æ•°æœ€é«˜: {best_f1_model['æ¨¡å‹']} ({best_f1_model['F1åˆ†æ•°']})\n")
    f.write(f"3. é¢„æµ‹æœ€å¿«: {fastest_model['æ¨¡å‹']} ({fastest_model['é¢„æµ‹é€Ÿåº¦(ms/æ ·æœ¬)']}ms/æ ·æœ¬)\n\n")

    f.write("å››ã€ç»“è®ºä¸å»ºè®®\n")
    f.write("1. å¯¹äºç”µå•†è¯„è®ºåˆ†ç±»ä»»åŠ¡ï¼ŒCNNå’ŒLSTMæ¨¡å‹è¡¨ç°è¾ƒå¥½\n")
    f.write("2. MLPæ¨¡å‹é€Ÿåº¦æœ€å¿«ï¼Œé€‚åˆå®æ—¶åº”ç”¨\n")
    f.write("3. Transformeræ¨¡å‹åœ¨è¶³å¤Ÿæ•°æ®ä¸‹æ½œåŠ›æœ€å¤§\n")
    f.write("4. æ¨èå®é™…åº”ç”¨ä¸­ä½¿ç”¨CNNæ¨¡å‹ï¼Œå¹³è¡¡æ€§èƒ½ä¸é€Ÿåº¦\n")

print("âœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ° 'ç”µå•†è¯„è®ºåˆ†ç±»_å®éªŒæŠ¥å‘Š.txt'")

# æœ€ç»ˆæ€»ç»“è¡¨æ ¼
print("\n" + "=" * 60)
print("æœ€ç»ˆå¯¹æ¯”ç»“æœï¼ˆç®€åŒ–ç‰ˆï¼‰")
print("=" * 60)

print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print("â”‚    æ¨¡å‹     â”‚  å‡†ç¡®ç‡    â”‚  F1åˆ†æ•°  â”‚  é¢„æµ‹é€Ÿåº¦  â”‚  å‚æ•°é‡    â”‚")
print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

for _, row in results_df.iterrows():
    # æ ‡è®°æœ€ä½³å€¼
    acc_str = f"{row['éªŒè¯å‡†ç¡®ç‡(%)']:.1f}%"
    f1_str = f"{row['F1åˆ†æ•°']:.3f}"
    speed_str = f"{row['é¢„æµ‹é€Ÿåº¦(ms/æ ·æœ¬)']:.2f}ms"
    param_str = f"{row['å‚æ•°é‡(M)']:.2f}M"

    if row['éªŒè¯å‡†ç¡®ç‡(%)'] == results_df['éªŒè¯å‡†ç¡®ç‡(%)'].max():
        acc_str = "â˜…" + acc_str
    if row['F1åˆ†æ•°'] == results_df['F1åˆ†æ•°'].max():
        f1_str = "â˜…" + f1_str
    if row['é¢„æµ‹é€Ÿåº¦(ms/æ ·æœ¬)'] == results_df['é¢„æµ‹é€Ÿåº¦(ms/æ ·æœ¬)'].min():
        speed_str = "âš¡" + speed_str

    print(f"â”‚ {row['æ¨¡å‹']:^11} â”‚ {acc_str:^10} â”‚ {f1_str:^8} â”‚ {speed_str:^10} â”‚ {param_str:^10} â”‚")

print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
print("\næ³¨: â˜…è¡¨ç¤ºè¯¥é¡¹æŒ‡æ ‡æœ€ä½³ï¼Œâš¡è¡¨ç¤ºé¢„æµ‹é€Ÿåº¦æœ€å¿«")
print("\nğŸ‰ å®éªŒå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶ã€‚")

# è¾“å‡ºä¸€äº›ç¤ºä¾‹é¢„æµ‹
print("\n" + "=" * 60)
print("ç¤ºä¾‹é¢„æµ‹")
print("=" * 60)

# é€‰æ‹©å‡ ä¸ªç¤ºä¾‹æ–‡æœ¬
sample_texts = X_val[:5]
sample_labels = y_val[:5]

print("å‰5ä¸ªéªŒè¯é›†æ ·æœ¬çš„é¢„æµ‹ç»“æœ:")
print("-" * 60)

for i, (text, true_label) in enumerate(zip(sample_texts, sample_labels)):
    # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹
    if best_acc_model['æ¨¡å‹'] == 'MLP':
        # è½¬æ¢ä¸ºTF-IDFç‰¹å¾
        text_tfidf = vectorizer_tfidf.transform([text]).toarray()
        input_tensor = torch.FloatTensor(text_tfidf).to(device)
        with torch.no_grad():
            output = trained_mlp(input_tensor)
            predicted = output.argmax().item()
    else:
        # è½¬æ¢ä¸ºåºåˆ—
        text_seq = text_to_sequence(text, max_sequence_len)
        input_tensor = torch.LongTensor([text_seq]).to(device)

        if best_acc_model['æ¨¡å‹'] == 'CNN':
            with torch.no_grad():
                output = trained_cnn(input_tensor)
                predicted = output.argmax().item()
        elif best_acc_model['æ¨¡å‹'] == 'LSTM':
            with torch.no_grad():
                output = trained_lstm(input_tensor)
                predicted = output.argmax().item()
        else:
            with torch.no_grad():
                output = trained_transformer(input_tensor)
                predicted = output.argmax().item()

    true_label_str = "å¥½è¯„" if true_label == 1 else "å·®è¯„"
    predicted_str = "å¥½è¯„" if predicted == 1 else "å·®è¯„"
    correct = "âœ“" if predicted == true_label else "âœ—"

    print(f"æ ·æœ¬ {i + 1}:")
    print(f"  æ–‡æœ¬: {text[:50]}...")
    print(f"  çœŸå®æ ‡ç­¾: {true_label_str}")
    print(f"  é¢„æµ‹æ ‡ç­¾: {predicted_str} {correct}")
    print("-" * 40)
